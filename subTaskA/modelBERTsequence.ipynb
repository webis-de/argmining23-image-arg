{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss139\\anaconda3\\envs\\transformer\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n",
    "from bertTokenization import preprocess_tweets\n",
    "from classifier import load_model\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch.optim import ASGD\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the below line to select the specific dataset topic for training the model\n",
    "# ---Gun Control Dataset---\n",
    "traindf = pd.read_csv(\"D:\\ASProject\\Transformer\\imageArg\\gun_control_train.csv\")\n",
    "valdf = pd.read_csv(\"D:\\ASProject\\Transformer\\imageArg\\gun_control_dev.csv\")\n",
    "#---abortion dataset---\n",
    "#traindf = pd.read_csv(r\"D:\\ASProject\\Transformer\\imageArg\\abortion_train.csv\")\n",
    "#valdf = pd.read_csv(r\"D:\\ASProject\\Transformer\\imageArg\\abortion_dev.csv\")\n",
    "#---combined dataset---\n",
    "#traindf_gc = pd.read_csv(\"D:\\ASProject\\Transformer\\imageArg\\gun_control_train.csv\")\n",
    "#valdf_gc = pd.read_csv(\"D:\\ASProject\\Transformer\\imageArg\\gun_control_dev.csv\")\n",
    "#traindf_ab = pd.read_csv(r\"D:\\ASProject\\Transformer\\imageArg\\abortion_train.csv\")\n",
    "#valdf_ab = pd.read_csv(r\"D:\\ASProject\\Transformer\\imageArg\\abortion_dev.csv\")\n",
    "#combined_train = pd.concat([traindf_gc, traindf_ab], ignore_index=True)\n",
    "#combined_val = pd.concat([valdf_gc, valdf_ab], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['tweet_url', 'persuasiveness', 'split']\n",
    "traindf.drop(columns=columns_to_drop, inplace=True)\n",
    "traindf['stance'] = traindf['stance'].replace({'oppose': 0, 'support': 1})\n",
    "valdf.drop(columns=columns_to_drop, inplace=True)\n",
    "valdf['stance'] = valdf['stance'].replace({'oppose': 0, 'support': 1})\n",
    "# for combined data\n",
    "#combined_train.drop(columns=columns_to_drop, inplace=True)\n",
    "#combined_train['stance'] = combined_train['stance'].replace({'oppose': 0, 'support': 1})\n",
    "#combined_val.drop(columns=columns_to_drop, inplace=True)\n",
    "#combined_val['stance'] = combined_val['stance'].replace({'oppose': 0, 'support': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the tweet_text column in the DataFrame\n",
    "# Commenting out while training the model for the raw tweettext.\n",
    "from dataCleaningScript import appriviation_converter\n",
    "from dataCleaningScript import clean_text\n",
    "traindf['tweet_text'] = traindf['tweet_text'].apply(appriviation_converter)\n",
    "traindf['tweet_text'] = traindf['tweet_text'].apply(clean_text)\n",
    "valdf['tweet_text'] = valdf['tweet_text'].apply(appriviation_converter)\n",
    "valdf['tweet_text'] = valdf['tweet_text'].apply(clean_text)\n",
    "\n",
    "# for combined data\n",
    "#combined_train['tweet_text'] = combined_train['tweet_text'].apply(appriviation_converter)\n",
    "#combined_train['tweet_text'] = combined_train['tweet_text'].apply(clean_text)\n",
    "#combined_val['tweet_text'] = combined_val['tweet_text'].apply(appriviation_converter)\n",
    "#combined_val['tweet_text'] = combined_val['tweet_text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = traindf.tweet_text.values\n",
    "trian_labels = traindf.stance.values\n",
    "val_tweets = valdf.tweet_text.values\n",
    "val_labels = valdf.stance.values\n",
    "# for combined data\n",
    "#train_tweets = combined_train.tweet_text.values\n",
    "#trian_labels = combined_train.stance.values\n",
    "#val_tweets = combined_val.tweet_text.values\n",
    "#val_labels = combined_val.stance.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized the tweet_texts and labels using the preprocess_tweets function.\n",
    "# the function tokenized the text\n",
    "input_ids_tr, attention_masks_tr, labels_tr = preprocess_tweets(train_tweets, trian_labels)\n",
    "input_ids_v, attention_masks_v, labels_v = preprocess_tweets(val_tweets, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  921 training samples\n",
      "   98 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(input_ids_tr, attention_masks_tr, labels_tr)\n",
    "val_dataset = TensorDataset(input_ids_v, attention_masks_v, labels_v)\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(val_dataset)\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it here.\n",
    "batch_size = 2\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly, This means that the training samples will be shuffled randomly in each epoch\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Loading the updated model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are considering different \"Optimizers\" and \"Learning Rate\" as the hyperparameters.\n",
    "# Adam optimizer that incorporates weight decay (L2 regularization) during optimization.\n",
    "# It helps prevent overfitting by adding a penalty term to the loss function that encourages smaller weights.\n",
    "    # 1. AdamW optimizer, and varying lr as 1e-5, 2e-5, 4e-5\n",
    "#optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "    # 2. Adam optimizer, and varying lr as 1e-5, 2e-5, 3e-5\n",
    "optimizer = Adam(model.parameters(), lr = 3e-5, eps = 1e-8)\n",
    "    # 3. SGD optimizer, and varying lr as 1e-2, 2e-2, 3e-2\n",
    "#optimizer = SGD(model.parameters(), lr=3e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning the model\n",
    "# Number of training epochs.\n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 8\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "# training step corresponds to one update of the model's parameters using a batch of training data.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "#The scheduler adjusts the learning rate during training, usually starting with a warm-up phase and then applying a linear decay schedule.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() #comparing the index with the maximum value in preds to the corresponding index in labels\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss using the datetime.timedelta function.\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 8 ========\n",
      "  Average Training Loss: 0.68 || Training epoch took: 0:01:31\n",
      " Validation Accuracy: 0.91 || F1 Score: 0.88 || Validation took: 0:00:02\n",
      "======== Epoch 2 / 8 ========\n",
      "  Average Training Loss: 0.39 || Training epoch took: 0:01:29\n",
      " Validation Accuracy: 0.86 || F1 Score: 0.82 || Validation took: 0:00:02\n",
      "======== Epoch 3 / 8 ========\n",
      "  Average Training Loss: 0.15 || Training epoch took: 0:01:30\n",
      " Validation Accuracy: 0.92 || F1 Score: 0.90 || Validation took: 0:00:02\n",
      "======== Epoch 4 / 8 ========\n",
      "  Average Training Loss: 0.05 || Training epoch took: 0:01:28\n",
      " Validation Accuracy: 0.93 || F1 Score: 0.90 || Validation took: 0:00:02\n",
      "======== Epoch 5 / 8 ========\n",
      "  Average Training Loss: 0.02 || Training epoch took: 0:01:30\n",
      " Validation Accuracy: 0.95 || F1 Score: 0.93 || Validation took: 0:00:02\n",
      "======== Epoch 6 / 8 ========\n",
      "  Average Training Loss: 0.00 || Training epoch took: 0:01:30\n",
      " Validation Accuracy: 0.95 || F1 Score: 0.93 || Validation took: 0:00:02\n",
      "======== Epoch 7 / 8 ========\n",
      "  Average Training Loss: 0.00 || Training epoch took: 0:01:28\n",
      " Validation Accuracy: 0.94 || F1 Score: 0.92 || Validation took: 0:00:02\n",
      "======== Epoch 8 / 8 ========\n",
      "  Average Training Loss: 0.00 || Training epoch took: 0:01:28\n",
      " Validation Accuracy: 0.95 || F1 Score: 0.93 || Validation took: 0:00:02\n",
      "Training complete!\n",
      "Total training took 0:12:28 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "seed_val = 42\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# Initialize best_eval_accuracy to a very low value before starting the training loop.\n",
    "best_eval_accuracy = 0\n",
    "best_f1_score=0\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    #               Training\n",
    "    # Perform one full pass over the training set.\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time() #track the start time of the epoch\n",
    "    total_train_loss = 0\n",
    "    model.train() #The model is set to training mode\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        # As we unpack the batch, we'll also copy each tensor to the device using the \n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # clear the gradients of the model parameters from the previous batch before computing the gradients for the current batch.\n",
    "        # Clearing the gradients is essential to avoid unwanted gradient accumulation, especially when using mini-batches during training. \n",
    "        # No gradient command, result in incorrect gradient updates and potential convergence issues.\n",
    "        optimizer.zero_grad()\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)        \n",
    "        loss = output.loss\n",
    "        total_train_loss += loss.item() # Method in PyTorch that returns the loss value as a Python scalar.\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"  Average Training Loss: {0:.2f} || Training epoch took: {1}\".format(avg_train_loss, training_time))\n",
    "    #               Validation\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    t0 = time.time() \n",
    "    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    #best_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device) #The input tensors are moved to the specified device.\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # Telling pytorch not to bother with constructing the compute graph during the forward pass, since this is only needed for backpropagation (training).\n",
    "        with torch.no_grad():        \n",
    "            output= model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = output.loss\n",
    "        total_eval_loss += loss.item() # Method in PyTorch that returns the loss value as a Python scalar.\n",
    "        # Move logits and labels to CPU if we are using GPU\n",
    "        logits = output.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        f1 = f1_score(label_ids, np.argmax(logits, axis=1), average='macro')\n",
    "        total_eval_f1 += f1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    if avg_val_f1 >= best_f1_score:\n",
    "        torch.save(model, 'bert_model_gc')\n",
    "        #torch.save(model, 'bert_model_ab')\n",
    "        #torch.save(model, 'bert_model_combined')\n",
    "        best_f1_score = avg_val_f1\n",
    "    print(\" Validation Accuracy: {0:.2f} || F1 Score: {1:.2f} || Validation took: {2}\".format(avg_val_accuracy, avg_val_f1, validation_time))\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. F1': avg_val_f1,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('bert_model_gc')\n",
    "#model = torch.load('bert_model_ab')\n",
    "#model = torch.load('bert_model_combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "# comment out the below line to test the gun control or abortion dataset\n",
    "# -- Gun control test\n",
    "df_test = pd.read_csv(\"gun_control_test.csv\")\n",
    "# -- Abortion test\n",
    "#df_test = pd.read_csv(\"abortion_test.csv\")\n",
    "# -- Combined test\n",
    "#gc_test = pd.read_csv(\"gun_control_test.csv\")\n",
    "#ab_test = pd.read_csv(\"abortion_test.csv\")\n",
    "#df_test = pd.concat([gc_test, ab_test], ignore_index=True)\n",
    "\n",
    "columns_to_drop = ['tweet_url','split']\n",
    "df_test.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# commented the text cleaning while testing on the raw dataset.\n",
    "df_test['tweet_text'] = df_test['tweet_text'].apply(appriviation_converter)\n",
    "df_test['tweet_text'] = df_test['tweet_text'].apply(clean_text)\n",
    "test_tweets = df_test.tweet_text.values\n",
    "\n",
    "\n",
    "# Call the preprocess_tweets function to tokenized the tweets and labels\n",
    "test_input_ids, test_attention_masks = preprocess_tweets(test_tweets)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "predicted_labels = []\n",
    "for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        with torch.no_grad():        \n",
    "            output= model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask)\n",
    "            logits = output.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            \n",
    "            predicted_labels.extend(list(pred_flat))\n",
    "\n",
    "df_output = pd.DataFrame()\n",
    "df_output['tweet_id'] = df_test['tweet_id']\n",
    "df_output['predicted_stance'] =predicted_labels\n",
    "df_output.to_csv('result_gc.csv',index=False)\n",
    "#df_output.to_csv('result_ab.csv',index=False) \n",
    "#df_output.to_csv('result_combined.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|                                               Results : Gun Control                                               |\n",
      "+-------------------+-------------------------+-------------------------+---------------------+---------------------+\n",
      "| Optimizer with LR | Cleaned Tweets Accuracy | Cleaned Tweets F1 Score | Raw Tweets Accuracy | Raw Tweets F1 Score |\n",
      "+-------------------+-------------------------+-------------------------+---------------------+---------------------+\n",
      "|   AdamW LR=1e-5   |           0.92          |           0.9           |         0.91        |         0.89        |\n",
      "|   AdamW LR=2e-5   |           0.93          |           0.91          |         0.93        |         0.91        |\n",
      "|   AdamW LR=4e-5   |           0.93          |           0.91          |         0.91        |         0.88        |\n",
      "|    Adam LR=1e-5   |           0.92          |           0.9           |         0.9         |         0.88        |\n",
      "|    Adam LR=2e-5   |           0.94          |           0.91          |         0.89        |         0.87        |\n",
      "|    Adam LR=3e-5   |           0.94          |           0.92          |         0.9         |         0.87        |\n",
      "|    SGD LR=1e-2    |           0.87          |           0.84          |         0.9         |         0.87        |\n",
      "|    SGD LR=2e-2    |           0.87          |           0.85          |         0.91        |         0.86        |\n",
      "|    SGD LR=3e-2    |           0.9           |           0.87          |         0.91        |         0.88        |\n",
      "+-------------------+-------------------------+-------------------------+---------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Sample data for the table - cleaned data\n",
    "cleaned_data = [\n",
    "    ['AdamW LR=1e-5', 0.92, 0.90],\n",
    "    ['AdamW LR=2e-5', 0.93, 0.91],\n",
    "    ['AdamW LR=4e-5', 0.93, 0.91],\n",
    "    ['Adam LR=1e-5', 0.92, 0.90],\n",
    "    ['Adam LR=2e-5', 0.94, 0.91],\n",
    "    ['Adam LR=3e-5', 0.94, 0.92],\n",
    "    ['SGD LR=1e-2', 0.87, 0.84],\n",
    "    ['SGD LR=2e-2', 0.87, 0.85],\n",
    "    ['SGD LR=3e-2', 0.90, 0.87],\n",
    "]\n",
    "\n",
    "# Sample data for the table - raw data\n",
    "raw_data = [\n",
    "    ['AdamW LR=1e-5', 0.91, 0.89],\n",
    "    ['AdamW LR=2e-5', 0.93, 0.91],\n",
    "    ['AdamW LR=4e-5', 0.91, 0.88],\n",
    "    ['Adam LR=1e-5', 0.90, 0.88],\n",
    "    ['Adam LR=2e-5', 0.89, 0.87],\n",
    "    ['Adam LR=3e-5', 0.90, 0.87],\n",
    "    ['SGD LR=1e-2', 0.90, 0.87],\n",
    "    ['SGD LR=2e-2', 0.91, 0.86],\n",
    "    ['SGD LR=3e-2', 0.91, 0.88],\n",
    "]\n",
    "\n",
    "# Create the PrettyTable and specify column names\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Optimizer with LR', 'Cleaned Tweets Accuracy', 'Cleaned Tweets F1 Score', 'Raw Tweets Accuracy', 'Raw Tweets F1 Score']\n",
    "\n",
    "# Add rows to the table\n",
    "for cleaned_row, raw_row in zip(cleaned_data, raw_data):\n",
    "    optimizer_lr_cleaned, acc_cleaned, f1_cleaned = cleaned_row\n",
    "    optimizer_lr_raw, acc_raw, f1_raw = raw_row\n",
    "    table.add_row([optimizer_lr_cleaned, acc_cleaned, f1_cleaned, acc_raw, f1_raw])\n",
    "\n",
    "# Set the title of the table\n",
    "table.title = \"Results : Gun Control\"\n",
    "\n",
    "# Display the table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                 Results : Abortion                                                |\n",
      "+-------------------+-------------------------+-------------------------+---------------------+---------------------+\n",
      "| Optimizer with LR | Cleaned Tweets Accuracy | Cleaned Tweets F1 Score | Raw Tweets Accuracy | Raw Tweets F1 Score |\n",
      "+-------------------+-------------------------+-------------------------+---------------------+---------------------+\n",
      "|   AdamW LR=1e-5   |           0.91          |           0.88          |         0.92        |         0.89        |\n",
      "|   AdamW LR=2e-5   |           0.93          |           0.9           |         0.92        |         0.88        |\n",
      "|   AdamW LR=3e-5   |           0.91          |           0.89          |         0.92        |         0.89        |\n",
      "|    Adam LR=1e-5   |           0.91          |           0.89          |         0.9         |         0.82        |\n",
      "|    Adam LR=2e-5   |           0.92          |           0.89          |         0.93        |         0.9         |\n",
      "|    Adam LR=3e-5   |           0.91          |           0.88          |         0.9         |         0.87        |\n",
      "|    SGD LR=1e-2    |           0.89          |           0.87          |         0.9         |         0.88        |\n",
      "|    SGD LR=2e-2    |           0.91          |           0.89          |         0.9         |         0.88        |\n",
      "|    SGD LR=3e-2    |           0.94          |           0.91          |         0.92        |         0.89        |\n",
      "+-------------------+-------------------------+-------------------------+---------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Sample data for the table - cleaned data\n",
    "cleaned_data = [\n",
    "    ['AdamW LR=1e-5', 0.91, 0.88],\n",
    "    ['AdamW LR=2e-5', 0.93, 0.90],\n",
    "    ['AdamW LR=3e-5', 0.91, 0.89],\n",
    "    ['Adam LR=1e-5', 0.91, 0.89],\n",
    "    ['Adam LR=2e-5', 0.92, 0.89],\n",
    "    ['Adam LR=3e-5', 0.91, 0.88],\n",
    "    ['SGD LR=1e-2', 0.89, 0.87],\n",
    "    ['SGD LR=2e-2', 0.91, 0.89],\n",
    "    ['SGD LR=3e-2', 0.94, 0.91],\n",
    "]\n",
    "\n",
    "# Sample data for the table - raw data\n",
    "raw_data = [\n",
    "    ['AdamW LR=1e-5', 0.92, 0.89],\n",
    "    ['AdamW LR=2e-5', 0.92, 0.88],\n",
    "    ['AdamW LR=4e-5', 0.92, 0.89],\n",
    "    ['Adam LR=1e-5', 0.90, 0.82],\n",
    "    ['Adam LR=2e-5', 0.93, 0.90],\n",
    "    ['Adam LR=3e-5', 0.90, 0.87],\n",
    "    ['SGD LR=1e-2', 0.90, 0.88],\n",
    "    ['SGD LR=2e-2', 0.90, 0.88],\n",
    "    ['SGD LR=3e-2', 0.92, 0.89],\n",
    "]\n",
    "\n",
    "# Create the PrettyTable and specify column names\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Optimizer with LR', 'Cleaned Tweets Accuracy', 'Cleaned Tweets F1 Score', 'Raw Tweets Accuracy', 'Raw Tweets F1 Score']\n",
    "\n",
    "# Add rows to the table\n",
    "for cleaned_row, raw_row in zip(cleaned_data, raw_data):\n",
    "    optimizer_lr_cleaned, acc_cleaned, f1_cleaned = cleaned_row\n",
    "    optimizer_lr_raw, acc_raw, f1_raw = raw_row\n",
    "    table.add_row([optimizer_lr_cleaned, acc_cleaned, f1_cleaned, acc_raw, f1_raw])\n",
    "\n",
    "# Set the title of the table\n",
    "table.title = \"Results : Abortion\"\n",
    "\n",
    "# Display the table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from the highest F1 score models of each topic are combined in the result.csv file.\n",
    "result_gc = pd.read_csv(\"result_gc.csv\")\n",
    "result_ab = pd.read_csv(\"result_ab.csv\")\n",
    "result = result_gc.append(result_ab, ignore_index=True)\n",
    "\n",
    "# -- results using the combined model\n",
    "#result = pd.read_csv(\"result_combined.csv\")\n",
    "\n",
    "result.rename(columns={result.columns[1]: \"stance\"}, inplace=True)\n",
    "result[\"stance\"] = result[\"stance\"].replace({0: \"oppose\", 1: \"support\"})\n",
    "\n",
    "result.to_csv('team.bertmodel.TaskA.1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Argumentative Stance (AS) Classification:**\n",
    "\n",
    "We used BertForSequenceClassification model to classify the stance of the tweet text as either support or oppose.\n",
    "\n",
    "**Implementation steps:**\n",
    "\n",
    "*Tokenization and Data Loading*: Tokenize the tweets using the BERT tokenizer. It encodes the tweets by adding special tokens [CLS] and [SEP], pads/truncates them to the maximum token length, and creates attention masks. We created a function that Tokenize the tweets and returns three PyTorch tensors: input_ids, attention_masks, and labels, which represent the tokenized input, attention masks, and labels, respectively.  These tensors are combined into a PyTorch TensorDataset which is then split into training and validation set using the random_split function. Then we provided batches of size 2 to the TensorDataset using PyTorch DataLoader.\n",
    "\n",
    "*Model Initialization*: Loads a pre-trained BERT model for sequence classification using the BertForSequenceClassification which is a 12 transformer layers BERT model. The base BERT model is extended by adding a classification layer on top. This additional layer is a linear transformation that takes the final hidden state of the BERT model and maps it to the 2 output labels considering the task as binary classification. The model by default, it uses a softmax activation function to produce probabilities for each class.\n",
    "\n",
    "*Training*: Perform training for a specified number of epochs. For each epoch, the training data is passed through the model, and gradients are calculated to update the model's parameters using the optimizer. A learning rate scheduler is also used.\n",
    "\n",
    "*Validation*: After each epoch, the model's performance is evaluated on the validation set. The accuracy and loss are computed, and the best model is saved based on validation accuracy.\n",
    "\n",
    "*Evaluation*: Optimizer and learning rate were considered as hyperparameters. Three different optimizers (Adam, AdamW, and SGD) were used, each with various learning rates. The validation accuracy and F1 score were recorded for both the cleaned tweets and raw tweets of each topic, respectively.\n",
    "\n",
    "The saved model with the highest F1 score for each topic was considered and tested on the respective test datasets separately. The \"result.csv\" file contains the combination of results for both the Gun Control and Abortion topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
